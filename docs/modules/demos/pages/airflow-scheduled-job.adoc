= airflow-scheduled-job
:description: This demo installs Airflow with Postgres and Redis on Kubernetes, showcasing DAG scheduling, job runs, and status verification via the Airflow UI.

Install this demo on an existing Kubernetes cluster:

[source,console]
----
$ stackablectl demo install airflow-scheduled-job
----

[WARNING]
====
This demo should not be run alongside other demos.
====

[#system-requirements]
== System requirements

To run this demo, your system needs at least:

* 2.5 https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#cpu[cpu units] (core/hyperthread)
* 10GiB memory
* 24GiB disk storage

== Overview

This demo will

* Install the required Stackable operators
* Spin up the following data products
** *Postgresql*: An open-source database used for Airflow cluster and job metadata.
** *Redis*: An in-memory data structure store used for queuing Airflow jobs
** *Airflow*: An open-source workflow management platform for data engineering pipelines.
* Mount two Airflow jobs (referred to as Directed Acyclic Graphs, or DAGs) for the cluster to use
* Enable and schedule the jobs
* Verify the job status with the Airflow Webserver UI

You can see the deployed products and their relationship in the following diagram:

image::airflow-scheduled-job/overview.png[]

== List deployed Stackable services

To list the installed Stackable services run the following command:

[source,console]
----
$ stackablectl stacklet list
┌─────────┬─────────┬───────────┬─────────────────────────────────────────┬─────────────────────────────────┐
│ PRODUCT ┆ NAME    ┆ NAMESPACE ┆ ENDPOINTS                               ┆ CONDITIONS                      │
╞═════════╪═════════╪═══════════╪═════════════════════════════════════════╪═════════════════════════════════╡
│ airflow ┆ airflow ┆ default   ┆ webserver-http  http://172.19.0.5:30913 ┆ Available, Reconciling, Running │
└─────────┴─────────┴───────────┴─────────────────────────────────────────┴─────────────────────────────────┘
----

include::partial$instance-hint.adoc[]

== Airflow Webserver UI

Open the `airflow` endpoint `webserver-airflow` in your browser (`http://172.19.0.5:30913` in this case).

image::airflow-scheduled-job/airflow_1.png[]

Log in with the username `admin` and password `adminadmin`.
Click in 'Active DAGs' at the top and you will see an overview showing the DAGs mounted during the demo
setup (`date_demo` and `sparkapp_dag`).

image::airflow-scheduled-job/airflow_2.png[]

There are two things to notice here.
Both DAGs have been enabled, as shown by the slider on the far right of the screen for each DAG
(DAGs are all `paused` initially and can be activated manually in the UI or via a REST call, as done in the setup for this demo):

image::airflow-scheduled-job/airflow_3.png[]

Secondly, the `date_demo` job has been busy, with several runs already logged.
The `sparkapp_dag` has only been run once because they have been defined with different schedules.

image::airflow-scheduled-job/airflow_4.png[]

Clicking on the DAG name and then on `Runs` will display the individual job runs:

image::airflow-scheduled-job/airflow_5.png[]

The `demo_date` job is running every minute.
With Airflow, DAGs can be started manually or scheduled to run when certain conditions are fulfilled - in this case, the DAG has been set up to run using a cron table, which is part of the DAG definition.

=== `demo_date` DAG

Let's drill down a bit deeper into this DAG.
At the top under the DAG name there is some scheduling information, which tells us that this job will run every minute continuously:

image::airflow-scheduled-job/airflow_6.png[]

Click on one of the job runs in the list to display the details for the task instances.
In the left-side pane the DAG is displayed either as a graph (this job is so simple that it only has one step, called `run_every_minute`), or as a "bar chart" showing each run.

image::airflow-scheduled-job/airflow_7.png[]

Click on the `run_every_minute` box in the centre of the page to select the logs:

[WARNING]
====
In this demo, the logs are not available when the KubernetesExecutor is deployed.
See the https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/kubernetes.html#managing-dags-and-logs[Airflow Documentation] for more details.

If you are interested in persisting the logs, take a look at the xref:logging.adoc[] demo.
====

image::airflow-scheduled-job/airflow_8.png[]

To look at the actual DAG code click on `Code`.
Here we can see the crontab information used to schedule the job as well the `bash` command that provides the output:

image::airflow-scheduled-job/airflow_9.png[]

=== `sparkapp_dag` DAG

Go back to DAG overview screen.
The `sparkapp_dag` job has a scheduled entry of `None` and a last-execution time.
This allows a DAG to be executed exactly once, with neither schedule-based runs nor any
https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dag-run.html#backfill[backfill].
The DAG can always be triggered manually again via REST or from within the Webserver UI.

image::airflow-scheduled-job/airflow_10.png[]

By navigating to the graphical overview of the job we can see that DAG has two steps, one to start the job - which runs
asynchronously - and another to poll the running job to report on its status.

image::airflow-scheduled-job/airflow_11.png[]

== Patching Airflow to stress-test DAG parsing using relevant environment variables

By default, Airflow runs database intialization routines on start-up.
These check that an Admin user exists and that the database schema is up-to-date.
Since they are idempotent and invoke little overhead, they can be run safely each time for most environments.
If, however, it makes sense to deactivate this, it can be turned off by patching the running cluster with a resource definition such as this:

[source,yaml]
----
---
apiVersion: airflow.stackable.tech/v1alpha1
kind: AirflowCluster
metadata:
  name: airflow
spec:
  clusterConfig:
    databaseInitialization:
      enabled: false  # <1>
----
<1> Turn off the initialization routine by setting `databaseInitialization.enabled` to `false`

NOTE: The field `databaseInitialization.enabled` is `true` by default to be backwards-compatible.
A fresh Airflow cluster cannot be created with this field set to `false` as this results in missing metadata in the Airflow database.

WARNING: Setting `databaseInitialization.enabled` to `false` is an unsupported operation as subsequent updates to a running Airflow cluster can result in broken behaviour due to inconsistent metadata.
Only set `databaseInitialization.enabled` to `false` if you know what you are doing!

The demo also created a third DAG in the ConfigMap, called `dag_factory.py`, which was not mounted to the cluster and therefore does not appear in the UI.
This DAG can be used to create a number of individual DAGs on-the-fly, thus allowing a certain degree of stress-testing of the DAG scan/register steps (the generated DAGs themselves are trivial and so this approach will not really increase the burden of DAG _parsing_).
To include this in the list of DAGs (without removing the existing ones), an extra volumeMount is needed, as shown below.
The patch also sets some environment variables that can used to change the frequency of certain operations. The descriptions can be found here: https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html.

[source,yaml]
----
---
apiVersion: airflow.stackable.tech/v1alpha1
kind: AirflowCluster
metadata:
  name: airflow
spec:
  clusterConfig:
    databaseInitialization:
      enabled: false
    volumeMounts:
      - name: airflow-dags
        mountPath: /dags/dag_factory.py
        subPath: dag_factory.py
      - name: airflow-dags
        mountPath: /dags/date_demo.py
        subPath: date_demo.py
      - name: airflow-dags
        mountPath: /dags/pyspark_pi.py
        subPath: pyspark_pi.py
      - name: airflow-dags
        mountPath: /dags/pyspark_pi.yaml
        subPath: pyspark_pi.yaml
  webservers:
    roleGroups:
      default:
        envOverrides: &envOverrides
          AIRFLOW__CORE__DAGS_FOLDER: "/dags"
          AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL: "60"
          AIRFLOW__CORE__MIN_SERIALIZED_DAG_FETCH_INTERVAL: "60"
          AIRFLOW__DAG_PROCESSOR__MIN_FILE_PROCESS_INTERVAL: "60"
          AIRFLOW__DAG_PROCESSOR__PRINT_STATS_INTERVAL: "60"
          AIRFLOW_CONN_KUBERNETES_IN_CLUSTER: "kubernetes://?__extra__=%7B%22extra__kubernetes__in_cluster%22%3A+true%2C+%22extra__kubernetes__kube_config%22%3A+%22%22%2C+%22extra__kubernetes__kube_config_path%22%3A+%22%22%2C+%22extra__kubernetes__namespace%22%3A+%22%22%7D"
  kubernetesExecutors:
    envOverrides: *envOverrides
  schedulers:
    roleGroups:
      default:
        envOverrides: *envOverrides
----

== Summary

This demo showed how DAGs can be made available for Airflow, scheduled, run and then inspected with the Webserver UI.
