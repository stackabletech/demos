= nifi-kafka-druid-water-level-data
:description: Install this demo for a showcase of using Kafka, Druid and Superset to visualize water levels in across Germany.

:superset: https://superset.apache.org/docs/using-superset/creating-your-first-dashboard/#creating-charts-in-explore-view
:druid-tutorial: https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html#loading-data-with-the-data-loader
:k8s-cpu: https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#cpu
:pegelonline-rest: https://www.pegelonline.wsv.de/webservice/dokuRestapi
:pegelonline: https://www.pegelonline.wsv.de/webservice/ueberblick
:kcat: https://github.com/edenhill/kcat

Install this demo on an existing Kubernetes cluster:

[source,console]
----
$ stackablectl demo install nifi-kafka-druid-water-level-data
----

[CAUTION]
====
This demo only runs in the `default` namespace, as a `ServiceAccount` will be created. Additionally, we have to use the
FQDN service names (including the namespace), so that the used TLS certificates are valid.
====

[#system-requirements]
== System requirements

To run this demo, your system needs at least:

* 9 {k8s-cpu}[cpu units] (core/hyperthread)
* 42GiB memory
* 75GiB disk storage

== Overview

This demo will

* Install the required Stackable operators.
* Spin up the following data products:
** *Superset*: A modern data exploration and visualization platform. This demo utilizes Superset to retrieve data from
   Druid via SQL queries and build dashboards on top of that data.
** *Kafka*:  A distributed event streaming platform for high-performance data pipelines, streaming analytics and data
   integration. This demo uses it as an event streaming platform to stream the data in near real-time.
** *NiFi*:  An easy-to-use, robust system to process and distribute data. This demo uses it to fetch water-level data
   from the internet and ingest it into Kafka.
** *Druid*: A real-time database to power modern analytics applications. This demo uses it to ingest the near real-time
   data from Kafka, store it and enable access to the data via SQL.
** *MinIO*: A S3 compatible object store. This demo uses it as persistent storage for Druid to store all the data.
* Ingest water level data from the {pegelonline}[PEGELONLINE web service] into Kafka. The data contains measured water
  levels of different measuring stations all around Germany. If the web service is unavailable, this demo will not work,
  as it needs the web service to ingest the data.
** First, historical data from the last 30 days will be fetched and ingested.
** Afterwards, the demo will fetch the current measurement of every station approximately every two minutes and ingest it
   near-real-time into Kafka.
* Start a Druid ingestion job that ingests the data into the Druid instance.
* Create Superset dashboards for visualization of the data.

The whole data pipeline will have a very low latency, from putting a record into Kafka to showing up in the dashboard
charts. You can see the deployed products and their relationship in the following diagram:

image::nifi-kafka-druid-water-level-data/overview.png[]

== List the deployed Stackable services

To list the installed Stackable services run the following command:

[source,console]
----
$ stackablectl stacklet list

┌───────────┬───────────────┬───────────┬─────────────────────────────────────────────────────────────────────────────────────────────┬─────────────────────────────────┐
│ PRODUCT   ┆ NAME          ┆ NAMESPACE ┆ ENDPOINTS                                                                                   ┆ CONDITIONS                      │
╞═══════════╪═══════════════╪═══════════╪═════════════════════════════════════════════════════════════════════════════════════════════╪═════════════════════════════════╡
│ druid     ┆ druid         ┆ default   ┆ broker-https                                https://172.19.0.3:32165                        ┆ Available, Reconciling, Running │
│           ┆               ┆           ┆ coordinator-https                           https://172.19.0.6:31287                        ┆                                 │
│           ┆               ┆           ┆ router-https                                https://172.19.0.3:31965                        ┆                                 │
├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ kafka     ┆ kafka         ┆ default   ┆ broker-default-0-listener-broker-kafka-tls  172.19.0.3:31041                                ┆ Available, Reconciling, Running │
│           ┆               ┆           ┆ broker-default-0-listener-broker-metrics    172.19.0.3:31503                                ┆                                 │
│           ┆               ┆           ┆ broker-default-bootstrap-kafka-tls          172.19.0.3:30793                                ┆                                 │
│           ┆               ┆           ┆ broker-default-bootstrap-metrics            172.19.0.3:32540                                ┆                                 │
├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ nifi      ┆ nifi          ┆ default   ┆ node-https                                  https://172.19.0.6:32038                        ┆ Available, Reconciling, Running │
├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ superset  ┆ superset      ┆ default   ┆ node-http                                   http://172.19.0.3:30435                         ┆ Available, Reconciling, Running │
├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ zookeeper ┆ zookeeper     ┆ default   ┆ server-zk                                   zookeeper-server.default.svc.cluster.local:2282 ┆ Available, Reconciling, Running │
├╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ minio     ┆ minio-console ┆ default   ┆ http                                        http://172.19.0.5:30196                         ┆                                 │
└───────────┴───────────────┴───────────┴─────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────────────────────┘

----

include::partial$instance-hint.adoc[]

== NiFi

NiFi fetches water-level data from the internet and ingests it into Kafka in real time. This demo includes a workflow
("process group") that fetches the last 30 days of historical measurements and produces the records in Kafka. It also
keeps streaming near-real-time updates for every available measuring station.

=== View the testdata-generation Job

You can look at the ingestion job running in NiFi by opening the endpoint `https` from your `stackablectl stacklet list`
command output. You have to use the endpoint from your command output. In this case, it is https://172.19.0.6:32038.
Open it with your favourite browser. Suppose you get a warning regarding the self-signed certificate generated by the
xref:home:secret-operator:index.adoc[Secret Operator] (e.g. Warning: Potential Security Risk Ahead). In that case, you must
tell your browser to trust the website and continue.

image::nifi-kafka-druid-water-level-data/nifi_1.png[]

Log in with the username `admin` and password `adminadmin`.

image::nifi-kafka-druid-water-level-data/nifi_13.png[]

The NiFi workflow contains the `IngestWaterLevelsToKafka` Process Group. Enter the Process Group by double-clicking on it or
right-clicking and selecting `Enter Group`.

image::nifi-kafka-druid-water-level-data/nifi_2.png[]

As you can see, the Process Group consists of lots of components. It is split into three main components:

. On the left is the first part bulk-loading all the known stations
. In the middle is the second part bulk-loading the historical data for the last 30 days
. On the right is the third part iterating over all stations and emitting the current measurement in an endless loop

You can zoom in by using your mouse and mouse wheel.

image::nifi-kafka-druid-water-level-data/nifi_3.png[]
image::nifi-kafka-druid-water-level-data/nifi_4.png[]

The left workflows works as follows:

. The `Get station list` processors fetch the current list of stations as JSON via HTTP from the
  {pegelonline}[PEGELONLINE web service].
. `Produce stations records` takes the list and produces a Kafka record for every station into the topic `stations`.
. `SplitRecords` simultaneously takes the single FlowFile (NiFi Record) containing all the stations and creates a new
  FlowFile for every station.
. `Extract station_uuid` takes every FlowFile representing a station and extract the attribute `station_uuid` into the
  metadata of the FlowFile.
. `Get historic measurements` calls the {pegelonline}[PEGELONLINE web service] for every station and fetches the
  measurements of the last 30 days.
. `Add station_uuid` will add the attribute `station_uuid` to the JSON list of measurements returned from the
  {pegelonline}[PEGELONLINE web service], which is missing.
. `PublishKafka` finally emits every measurement as a Kafka record to the topic `measurements`.

The right side works similarly but is executed in an endless loop to stream the data in near-realtime. Double-click on
the `Get station list` processor to show the processor details.

image::nifi-kafka-druid-water-level-data/nifi_5.png[]

Head over to the `Properties` tab.

image::nifi-kafka-druid-water-level-data/nifi_6.png[]

Here, you can see the setting `HTTP URL`, which specifies the download URL from where the JSON file containing the
stations is retrieved. Close the processor details popup by clicking `Close`. You can also have a detailed view of the
`Produce station records` processor by double-clicking it.

image::nifi-kafka-druid-water-level-data/nifi_7.png[]

The Kafka connection details within this processor - like broker addresses and topic names - are specified. It uses the
`JsonTreeReader` to parse the downloaded JSON and the `JsonRecordSetWriter` to split it into individual JSON records
before writing it out. Double-click the `Get 30 days historic data` processor.

image::nifi-kafka-druid-water-level-data/nifi_8.png[]

This processor fetched the historical data for every station. Click on the `HTTP URL` property.

image::nifi-kafka-druid-water-level-data/nifi_9.png[]

The `HTTP URL` does contain the `$\{station_uuid\}` placeholder, which gets replaced for every station.

Double-click the `PublishKafka` processor.

image::nifi-kafka-druid-water-level-data/nifi_10.png[]

You can also see the number of produced records by right-clicking on `PublishKafka` and selecting
`View status history`.

image::nifi-kafka-druid-water-level-data/nifi_11.png[]

After selecting `Messages Sent (5 mins)` in the top right corner, you can see that ~670k records were produced in
~5 minutes, corresponding to ~22k measurements/s. Keep in mind that the demo uses a single-node NiFi setup. The
performance can be increased by using multiple nodes.

Regarding the NiFi resources, use the hamburger menu icon on the top right corner and select `Node Status History`.

image::nifi-kafka-druid-water-level-data/nifi_12.png[]

The diagram shows the used heap size of the NiFi node. You can also select other metrics to show in the top right
corner.

== Druid

Druid is used to ingest the near real-time data from Kafka, store it and enable SQL access. The demo has started two
ingestion jobs - one reading from the topic `stations` and the other from `measurements` - and saving it into Druid's
deep storage. The Druid deep storage is based on the S3 store provided by MinIO.

=== View the Ingestion job

You can have a look at the ingestion jobs running in Druid by opening the Druid endpoint `router-https` from your
`stackablectl stacklet list` command output (https://172.19.0.3:31965 in this case).

image::nifi-kafka-druid-water-level-data/druid_1.png[]

By clicking on `Supervisors` at the top you can see the running ingestion jobs.

image::nifi-kafka-druid-water-level-data/druid_2.png[]

After clicking on the magnification glass to the right side of the `RUNNING` supervisor, you can see additional
information (here, the supervisor `measurements` was chosen). On the tab `Task stats` on the left, you can see the
number of processed records as well as the number of errors.

image::nifi-kafka-druid-water-level-data/druid_3.png[]

The statistics show how many records that Druid ingested during the last minute and the total already ingested.
All records have been ingested successfully, indicated by having no `processWithError`, `thrownAway` or `unparseable` records in the output of the `View raw` button at the top right.

=== Query the Data Source

The started ingestion jobs have automatically created the Druid data sources `stations` and `measurements`. You can see
the available data sources by clicking on `Datasources` at the top.

image::nifi-kafka-druid-water-level-data/druid_4.png[]

The `Avg. row size (bytes)` column shows that a typical `measurement` record has `4` bytes, while a station record has
`222`, more than 50 times the size. So, by choosing two dedicated topics over a single topic, this demo saved 50x of
storage and computation costs.

By clicking on `segments` under `Availability` for the `measurements` data source, you can see the segments of the data source.
In this case, the `measurements` data source is partitioned by the measurement day, resulting in 32 segments.

image::nifi-kafka-druid-water-level-data/druid_5.png[]

Druid offers a web-based way of querying the data sources via SQL. To achieve this you first have to click on `Query` at
the top.

image::nifi-kafka-druid-water-level-data/druid_6.png[]

You can now enter any arbitrary SQL statement, to e.g. list 10 stations run

[source,sql]
----
select * from stations limit 10
----

image::nifi-kafka-druid-water-level-data/druid_7.png[]

To count the measurements per day run

[source,sql]
----
select
  time_format(__time, 'YYYY/MM/dd') as "day",
  count(*) as measurements
from measurements
group by 1
order by 1 desc
----

image::nifi-kafka-druid-water-level-data/druid_8.png[]

== Superset

Superset provides the ability to execute SQL queries and build dashboards. Open the Superset endpoint
`node-http` in your browser (http://172.19.0.3:30435 in this case).

image::nifi-kafka-druid-water-level-data/superset_1.png[]

Log in with the username `admin` and password `adminadmin`.

image::nifi-kafka-druid-water-level-data/superset_2.png[]

=== View the dashboard

The demo has created a Dashboard to visualize the water level data. To open it, click on the tab `Dashboards` at the top.

image::nifi-kafka-druid-water-level-data/superset_3.png[]

Click on the dashboard called `Water level data`. It might take some time until the dashboard renders all the included
charts.

image::nifi-kafka-druid-water-level-data/superset_4a.png[]

NOTE: The charts on the right (`Current water level deviation` and `Stations distribution`) are rendered without a background map, as this is dependent upon a mapbox API key, which cannot be hosted in a public repository. The figure below shows how this would look if the user has their own key:

image::nifi-kafka-druid-water-level-data/superset_4b.png[]

=== View the charts

The dashboard `Water level data` consists of multiple charts. To list the charts, click on the tab `Charts` at the top.

image::nifi-kafka-druid-water-level-data/superset_5.png[]

Click on the Chart `Measurements / hour`. On the left side, you can modify the chart and click on `Update Chart` to see the
effect.

image::nifi-kafka-druid-water-level-data/superset_6.png[]

// Comment out the next section as long as the mapbox api key is not active.
// === View the Station Distribution on the World Map

// To look at the stations' geographical distribution, you have to click on the tab `Charts` at the top again. Afterwards,
// click on the chart `Stations distribution`.

// image::nifi-kafka-druid-water-level-data/superset_7.png[]

// The stations are, of course, placed alongside waterways. They are coloured by the waters they measure, so all stations
// alongside a body of water have the same colour. You can move and zoom the map with your mouse to interactively explore
// the map. You can, e.g. have a detailed look at the water https://en.wikipedia.org/wiki/Rhine[Rhein].

// image::nifi-kafka-druid-water-level-data/superset_8.png[]

=== Execute arbitrary SQL statements

Within Superset, you can create dashboards and run arbitrary SQL statements. On the top, click on the tab `SQL` ->
`SQL Lab`.

image::nifi-kafka-druid-water-level-data/superset_9.png[]

On the left, select the database `druid`, the schema `druid` and set `See table schema` to `stations` or `measurements`.

image::nifi-kafka-druid-water-level-data/superset_10.png[]

In the right textbox, enter the desired SQL statement. We need to join the two tables to get exciting results. Run the
following query to determine the number of measurements the stations made:

[source,sql]
----
select
  stations.longname as station,
  count(*) as measurements
from measurements inner join stations on stations.uuid = measurements.station_uuid
group by 1
order by 2 desc
----

image::nifi-kafka-druid-water-level-data/superset_11.png[]

You can also find the number of measurements for every body of water:

[source,sql]
----
select
  stations.water_longname as water,
  count(*) as measurements
from measurements inner join stations on stations.uuid = measurements.station_uuid
group by 1
order by 2 desc
----

image::nifi-kafka-druid-water-level-data/superset_12.png[]

What might also be interesting is the average and current measurement of the stations:

[source,sql]
----
select
  stations.longname as station,
  avg("value") as avg_measurement,
  latest_by("value", measurements."__time") as current_measurement,
  latest_by("value", measurements."__time") - avg("value") as diff
from measurements inner join stations on stations.uuid = measurements.station_uuid
group by 1
order by 2 desc
----

image::nifi-kafka-druid-water-level-data/superset_13.png[]

== MinIO

The S3 MinIO store provides persistent deep storage for Druid to store all the data used. Open the MinIO endpoint
`http` retrieved by `stackablectl stacklet list` in your browser (http://172.19.0.5:30196 in this case).

image::nifi-kafka-druid-water-level-data/minio_1.png[]

Log in with the username `admin` and password `adminadmin`.

image::nifi-kafka-druid-water-level-data/minio_2.png[]

Click on the bucket `demo` and open the folder `data`.

image::nifi-kafka-druid-water-level-data/minio_3.png[]

You can see that Druid has created a folder for both data sources. Go ahead and open the folder `measurements`.

image::nifi-kafka-druid-water-level-data/minio_4.png[]

Druid saved 51.5 MiB of data within 33 prefixes (folders). One prefix corresponds to one segment, which contains all the
measurements of a day. If you don't see any folders or files, the reason is that Druid still needs to save its data from
memory to the deep storage. After waiting for roughly an hour, the data should have been flushed to S3 and show up.

image::nifi-kafka-druid-water-level-data/minio_5.png[]

If you open up a prefix for a specific day, you can see that Druid has placed a file containing that day's data there.

== Summary

The demo puts station records into the Kafka stream pipeline topic `station`.
Over time it will stream ~30,000 measurements/s for a total of ~11 million measurements into the topic `measurements`.
Druid ingested the data near real-time into its data source and enabled SQL access to it.
Superset was used as a web-based frontend to execute SQL statements and build dashboards.

== Where to go from here

There are multiple paths to go from here. The following sections give you some ideas on what to explore next. You can
find the description of the water level data on the {pegelonline-rest}[PEGELONLINE REST API documentation
(German only)].

=== Execute arbitrary SQL statements

Within Superset (or the Druid web interface), you can execute arbitrary SQL statements to explore the water level data.

=== Create additional dashboards

You also can create additional charts and bundle them together in a Dashboard. Have a look at
{superset}[the Superset documentation] on how to do that.

=== Load additional data

You can use the NiFi web interface to collect arbitrary data and write it to Kafka (it's recommended to use new Kafka
topics for that). Alternatively, you can use a Kafka client like {kcat}[kcat] to create new topics and ingest data.
Using the Druid web interface, you can start an ingestion job that consumes and stores the data in an internal data
source. There is an excellent {druid-tutorial}[tutorial] from Druid on how to do this. Afterwards, the data source can
be analyzed within Druid and Superset, like the water level data.
