{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6515406-dc52-4a2b-9ae8-99fff7773146",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "We can first output some versions that are running and read the minio credentials from the secret that has been mounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0705d7d-d93b-4e3b-bd49-2b6696ddc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -V\n",
    "! java --version\n",
    "! pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd941fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get minio credentials\n",
    "with open(\"/minio-s3-credentials/accessKey\", \"r\") as f:\n",
    "    minio_user = f.read().strip()\n",
    "\n",
    "with open(\"/minio-s3-credentials/secretKey\", \"r\") as f:\n",
    "    minio_pwd = f.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b5e14-c704-4408-a803-561cbcf8005f",
   "metadata": {},
   "source": [
    "## Spark\n",
    "Spark can be used in client mode (recommended for JupyterHub notebooks, as code is intended to be called in an interactive\n",
    "fashion), which is the default, or cluster mode. This notebook uses spark in client mode, meaning that the notebook itself\n",
    "acts as the driver. It is important that the versions of spark and python match across the driver (running in the juypyterhub image)\n",
    "and the executor(s) (running in a separate image, specified below with the `spark.kubernetes.container.image` setting).\n",
    "\n",
    "The jupyterhub image `quay.io/jupyter/pyspark-notebook:spark-3.5.2` uses a base ubuntu image (like the spark images).\n",
    "The versions of java match exactly. Python versions can differ at patch level, and the image used below `oci.stackable.tech/sandbox/spark:3.5.2-python311` is built from a `spark:3.5.2-scala2.12-java17-ubuntu` base image with python 3.11 (the same major/minor version as the notebook) installed.\n",
    "\n",
    "## S3\n",
    "As we will be reading data from an S3 bucket, we need to add the necessary `hadoop` and `aws` libraries in the same hadoop version as the\n",
    "notebook image (see `spark.jars.packages`), and define the endpoint settings (see `spark.hadoopo.fs.*`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606363ba-0c97-4156-af1c-c8ad54745cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "NAMESPACE = os.environ.get(\"NAMESPACE\", \"default\")\n",
    "POD_NAME = os.environ.get(\"HOSTNAME\", f\"jupyter-{os.environ.get('USER', 'default')}-{NAMESPACE}\")\n",
    "\n",
    "EXECUTOR_IMAGE = \"oci.stackable.tech/sandbox/spark:3.5.2-python311\" \n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(f\"k8s://https://{os.environ['KUBERNETES_SERVICE_HOST']}:{os.environ['KUBERNETES_SERVICE_PORT']}\")\n",
    "    .appName(f\"process-s3-{POD_NAME}\")\n",
    "    .config(\"spark.kubernetes.container.image\", EXECUTOR_IMAGE)\n",
    "    .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "    .config(\"spark.kubernetes.namespace\", NAMESPACE)\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "    .config(\"spark.kubernetes.authenticate.executor.serviceAccountName\", \"spark\")\n",
    "    .config(\"spark.driver.port\", \"2222\")\n",
    "    .config(\"spark.driver.blockManager.port\", \"7777\")\n",
    "    .config(\"spark.executor.instances\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"1g\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000/\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", minio_user)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", minio_pwd)\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-client-api:3.3.4,org.apache.hadoop:hadoop-client-runtime:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-common:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.162\")\n",
    "    .config(\"spark.kubernetes.driver.pod.name\", POD_NAME)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08096d-1f7a-4c95-8807-aca76290cdfa",
   "metadata": {},
   "source": [
    "### Create an in-memory DataFrame\n",
    "This will check that libraries across driver and executor are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1ab91-ab2e-49b0-a72f-164915e4ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"a\", 1), (\"b\", 2)], [\"col1\", \"col2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469988e4-1057-49f6-8c8f-93743c4a6839",
   "metadata": {},
   "source": [
    "### Check s3 with pyarrow\n",
    "As well as spark, we can inspect S3 buckets with the `pyarrow` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8479cb-f216-4a8f-b9db-6da17ffebaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual S3 file check via pyarrow.fs\n",
    "import pyarrow.fs as fs\n",
    "\n",
    "s3 = fs.S3FileSystem(endpoint_override=\"http://minio:9000/\", access_key=minio_user, secret_key=minio_pwd, scheme=\"http\")\n",
    "files = s3.get_file_info(fs.FileSelector(\"demo/gas-sensor/raw/\", recursive=True))\n",
    "for f in files:\n",
    "    print(\"Found file:\", f.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e3331-5587-40c5-8a38-a1c3527bb25a",
   "metadata": {},
   "source": [
    "### Read/Write operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3a://demo/gas-sensor/raw/\", header = True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35861943-7586-434f-a03d-31ebf03b59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(\"s3a://demo/gas-sensor/rewritten/\", mode=\"overwrite\")\n",
    "df.write.parquet(\"s3a://demo/gas-sensor/parquet/\", mode=\"overwrite\")\n",
    "\n",
    "df2 = spark.read.parquet(\"s3a://demo/gas-sensor/parquet/\", header = True)\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319fa38-96de-4c8a-96e0-8e47ef5a7561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions\n",
    "\n",
    "df2 = df2.withColumn(\"hour\", (functions.floor(df2.timesecs / 60) + 1))\n",
    "\n",
    "dfs = df2.select(\n",
    "    df2.hour,\n",
    "    df2.humidity,\n",
    "    df2.temperature,\n",
    "    df2.flowrate\n",
    ").groupby(\"hour\").agg(\n",
    "    functions.round(functions.avg('humidity'), 2).alias('humidity'),\n",
    "    functions.round(functions.avg('temperature'), 2).alias('temperature'),\n",
    "    functions.round(functions.avg('flowrate'), 2).alias('flowrate')\n",
    ").orderBy(\"hour\")\n",
    "\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.write.parquet(\"s3a://demo/gas-sensor/agg/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d38d8f-57f7-4629-a4d2-e28142cc6a68",
   "metadata": {},
   "source": [
    "### Convert between Spark and Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d68a92-c104-4cc6-9a89-c052324ba1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = dfs.toPandas()\n",
    "df_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128628ff-f2c7-4a04-8c1a-020b239e1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df_pandas)\n",
    "spark_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
