---
apiVersion: secrets.stackable.tech/v1alpha1
kind: TrustStore
metadata:
  name: truststore-pem
spec:
  secretClassName: tls
  format: tls-pem
  targetKind: ConfigMap
---
apiVersion: airflow.stackable.tech/v1alpha1
kind: AirflowCluster
metadata:
  name: airflow
spec:
  image:
    productVersion: 3.0.6
    pullPolicy: IfNotPresent
  clusterConfig:
    authorization:
      opa:
        configMapName: opa
        package: airflow
        cache:
          entryTimeToLive: 5s
          maxEntries: 10
    loadExamples: false
    credentialsSecret: airflow-credentials
    volumes:
      - name: airflow-dags
        configMap:
          name: airflow-dags
      - name: s3-tls-pem
        configMap:
          name: truststore-pem
    volumeMounts:
      - name: airflow-dags
        mountPath: /stackable/airflow/dags/dbt.py
        subPath: dbt.py
      - name: s3-tls-pem
        mountPath: /stackable/s3-tls-pem
  webservers:
    roleConfig:
      listenerClass: external-stable
    config:
      resources:
        cpu:
          min: "2"
          max: "3"
        memory:
          limit: 3Gi
    envOverrides: &envOverrides
      AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
      AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "s3_conn"
      AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://airflow/logs"
      AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: "FALSE"
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: adminadmin # {{ airflowAdminPassword }}
      AIRFLOW_CONN_S3_CONN: "{\"conn_type\": \"aws\", \"extra\": {\"endpoint_url\": \"https://minio.default.svc.cluster.local:9000\", \"verify\": \"/stackable/s3-tls-pem/ca.crt\"}}"
    roleGroups:
      default:
        replicas: 1
  kubernetesExecutors:
    envOverrides: *envOverrides
  schedulers:
    envOverrides: *envOverrides
    roleGroups:
      default:
        replicas: 1
  dagProcessors:
    envOverrides: *envOverrides
    roleGroups:
      default:
        replicas: 1
  triggerers:
    envOverrides: *envOverrides
    roleGroups:
      default:
        replicas: 1
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-dags
data:
  dbt.py: |
    from airflow import DAG
    from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
    from kubernetes.client import models as k8s
    from kubernetes.client import V1EnvVar, V1EnvVarSource, V1SecretKeySelector

    tls_volume = k8s.V1Volume(
        name="server-tls-mount",
        ephemeral=k8s.V1EphemeralVolumeSource(
            volume_claim_template=k8s.V1PersistentVolumeClaimTemplate(
                metadata=k8s.V1ObjectMeta(
                    annotations={
                        "secrets.stackable.tech/class": "trino-tls",
                        "secrets.stackable.tech/scope": "pod,node"
                    }
                ),
                spec=k8s.V1PersistentVolumeClaimSpec(
                    access_modes=["ReadWriteOnce"],
                    resources=k8s.V1ResourceRequirements(
                        requests={"storage": "1"}
                    ),
                    storage_class_name="secrets.stackable.tech"
                )
            )
        )
    )

    tls_volume_mount = k8s.V1VolumeMount(
      name="server-tls-mount", mount_path="/dbt/trusted"
    )

    pod_security_context = k8s.V1PodSecurityContext(
        fs_group=1000
    )

    with DAG(
        dag_id="run_dbt",
        schedule=None,
        tags=["Demo", "DBT"],
        catchup=False
    ) as dag:
      run_dbt = KubernetesPodOperator(
          image="oci.stackable.tech/sandbox/andrew/dbt-demo:0.0.1",
          image_pull_policy="IfNotPresent",
          cmds=["/bin/bash", "-x", "-euo", "pipefail", "-c"],
          arguments=["cd /dbt/dbt_test && export DBT_PROFILES_DIR=/dbt/dbt_test && dbt debug && dbt run && dbt test"],
          name="run-dbt",
          task_id="dbt-test",
          get_logs=True,
          volumes=[tls_volume],
          volume_mounts=[tls_volume_mount],
          env_vars=[
              V1EnvVar(
                  name="TRINO_PASSWORD",
                  value_from=V1EnvVarSource(
                      secret_key_ref=V1SecretKeySelector(
                          name="airflow-credentials",
                          key="adminUser.password"
                      )
                  )
              ),
              V1EnvVar(name="TRINO_USER", value="admin"),
              V1EnvVar(name="TRINO_HOST", value="trino-coordinator-default-headless.default.svc.cluster.local"),
              V1EnvVar(name="TRINO_PORT", value="8443"),
              V1EnvVar(name="CERT_PATH", value="/dbt/trusted/ca.crt"),
          ],
          security_context=pod_security_context,
          startup_timeout_seconds=600
      )
      run_dbt
---
apiVersion: v1
kind: Secret
metadata:
  name: airflow-credentials
type: Opaque
stringData:
  adminUser.username: admin
  adminUser.firstname: Airflow
  adminUser.lastname: Admin
  adminUser.email: airflow@airflow.com
  adminUser.password: {{ airflowAdminPassword }}
  connections.secretKey: "airflowSecretKey"
  connections.sqlalchemyDatabaseUri: postgresql+psycopg2://airflow:airflow@postgresql-airflow/airflow
  connections.celeryResultBackend: db+postgresql://airflow:airflow@postgresql-airflow/airflow
  connections.celeryBrokerUrl: redis://:redis@redis-airflow-master:6379/0
