---
# {% raw %}
apiVersion: airflow.stackable.tech/v1alpha1
kind: AirflowCluster
metadata:
  name: airflow
spec:
  image:
    productVersion: 3.0.6
    pullPolicy: IfNotPresent
  clusterConfig:
    authorization:
      opa:
        configMapName: opa
        package: airflow
        cache:
          entryTimeToLive: 5s
          maxEntries: 10
    loadExamples: false
    credentialsSecret: airflow-credentials
    volumes:
      - name: airflow-dags
        configMap:
          name: airflow-dags
      - name: tls-pem
        configMap:
          name: truststore-pem
    volumeMounts:
      - name: airflow-dags
        mountPath: /dags/date_demo.py
        subPath: date_demo.py
      - name: airflow-dags
        mountPath: /dags/pyspark_pi.py
        subPath: pyspark_pi.py
      - name: airflow-dags
        mountPath: /dags/pyspark_pi.yaml
        subPath: pyspark_pi.yaml
      - name: airflow-dags
        mountPath: /dags/kafka.py
        subPath: kafka.py
      - name: airflow-dags
        mountPath: /dags/triggerer.py
        subPath: triggerer.py
      - name: airflow-dags
        mountPath: /dags/dbt.py
        subPath: dbt.py
      - name: tls-pem
        mountPath: /stackable/tls-pem
  webservers:
    roleConfig:
      listenerClass: external-stable
    config:
      resources:
        cpu:
          min: "2"
          max: "3"
        memory:
          limit: 3Gi
    envOverrides: &envOverrides
      AIRFLOW__CORE__DAGS_FOLDER: "/dags"
      PYTHONPATH: "/stackable/app/log_config:/dags"
      AIRFLOW_CONN_KUBERNETES_IN_CLUSTER: "kubernetes://?__extra__=%7B%22extra__kubernetes__in_cluster%22%3A+true%2C+%22extra__kubernetes__kube_config%22%3A+%22%22%2C+%22extra__kubernetes__kube_config_path%22%3A+%22%22%2C+%22extra__kubernetes__namespace%22%3A+%22%22%7D"
      # Airflow 3: Disable decision caching for easy debugging
      AIRFLOW__CORE__AUTH_OPA_CACHE_MAXSIZE: "0"
      AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
      AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "s3_conn"
      AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://airflow/logs"
      AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: "FALSE"
    configOverrides:
      webserver_config.py:
        # Allow "POST /login/" without CSRF token
        WTF_CSRF_ENABLED: "False"
    podOverrides: &podOverrides
      spec:
        containers:
          - name: airflow
            image: oci.stackable.tech/sdp/airflow:3.0.6-stackable0.0.0-dev
            imagePullPolicy: IfNotPresent
            env:
              - name: NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: AWS_SECRET_ACCESS_KEY
                valueFrom:
                 secretKeyRef:
                   name: airflow-credentials
                   key: adminUser.password
              - name: AWS_ACCESS_KEY_ID
                value: admin
              - name: AIRFLOW_CONN_KAFKA_CONN
                value: '{"conn_type": "kafka", "extra": {"bootstrap.servers": "kafka-broker-default-0-listener-broker.$(NAMESPACE).svc.cluster.local:9093", "security.protocol": "SSL", "ssl.ca.location": "/stackable/tls-pem/ca.crt", "group.id": "airflow_group", "auto.offset.reset": "latest"}}'
              - name: AIRFLOW_CONN_S3_CONN
                value: '{"conn_type": "aws", "extra": {"endpoint_url": "https://minio.$(NAMESPACE).svc.cluster.local:9000", "verify": "/stackable/tls-pem/ca.crt"}}'
    roleGroups:
      default:
        replicas: 1
  kubernetesExecutors:
    # apply the podOverrides to the *base* container
    envOverrides: *envOverrides
    podOverrides:
      spec:
        containers:
          - name: base
            image: oci.stackable.tech/sdp/airflow:3.0.6-stackable0.0.0-dev
            imagePullPolicy: IfNotPresent
            env:
              - name: AWS_SECRET_ACCESS_KEY
                valueFrom:
                 secretKeyRef:
                   name: airflow-credentials
                   key: adminUser.password
              - name: AWS_ACCESS_KEY_ID
                value: admin
  schedulers:
    envOverrides: *envOverrides
    podOverrides: *podOverrides
    roleGroups:
      default:
        replicas: 1
  dagProcessors:
    envOverrides: *envOverrides
    podOverrides: *podOverrides
    roleGroups:
      default:
        replicas: 1
  triggerers:
    envOverrides: *envOverrides
    podOverrides: *podOverrides
    roleGroups:
      default:
        replicas: 1
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-dags
data:
  dbt.py: |
    from airflow import DAG
    from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
    from kubernetes.client import models as k8s
    from kubernetes.client import V1EnvVar, V1EnvVarSource, V1SecretKeySelector

    tls_volume = k8s.V1Volume(
        name="server-tls-mount",
        ephemeral=k8s.V1EphemeralVolumeSource(
            volume_claim_template=k8s.V1PersistentVolumeClaimTemplate(
                metadata=k8s.V1ObjectMeta(
                    annotations={
                        "secrets.stackable.tech/class": "trino-tls",
                        "secrets.stackable.tech/scope": "pod,node"
                    }
                ),
                spec=k8s.V1PersistentVolumeClaimSpec(
                    access_modes=["ReadWriteOnce"],
                    resources=k8s.V1ResourceRequirements(
                        requests={"storage": "1"}
                    ),
                    storage_class_name="secrets.stackable.tech"
                )
            )
        )
    )

    tls_volume_mount = k8s.V1VolumeMount(
      name="server-tls-mount", mount_path="/dbt/trusted"
    )

    pod_security_context = k8s.V1PodSecurityContext(
        fs_group=1000
    )

    with DAG(
        dag_id="run_dbt",
        schedule=None,
        tags=["Demo", "DBT"],
        catchup=False
    ) as dag:
      run_dbt = KubernetesPodOperator(
          image="oci.stackable.tech/sandbox/andrew/dbt-demo:0.0.1",
          image_pull_policy="IfNotPresent",
          cmds=["/bin/bash", "-x", "-euo", "pipefail", "-c"],
          arguments=["cd /dbt/dbt_test && export DBT_PROFILES_DIR=/dbt/dbt_test && dbt debug && dbt run && dbt test"],
          name="run-dbt",
          task_id="dbt-test",
          get_logs=True,
          volumes=[tls_volume],
          volume_mounts=[tls_volume_mount],
          env_vars=[
              V1EnvVar(
                  name="TRINO_PASSWORD",
                  value_from=V1EnvVarSource(
                      secret_key_ref=V1SecretKeySelector(
                          name="airflow-credentials",
                          key="adminUser.password"
                      )
                  )
              ),
              V1EnvVar(name="TRINO_USER", value="admin"),
              V1EnvVar(name="TRINO_HOST", value="trino-coordinator-default-headless.default.svc.cluster.local"),
              V1EnvVar(name="TRINO_PORT", value="8443"),
              V1EnvVar(name="CERT_PATH", value="/dbt/trusted/ca.crt"),
          ],
          security_context=pod_security_context,
          startup_timeout_seconds=600
      )
      run_dbt

  kafka.py: |
    from airflow.providers.apache.kafka.triggers.msg_queue import KafkaMessageQueueTrigger
    from airflow.providers.standard.operators.empty import EmptyOperator
    from airflow.sdk import DAG, Asset, AssetWatcher

    import logging
    logger = logging.getLogger(__name__)

    logger.info("✅ kafka.apply_function module imported")

    def apply_function(message):
      try:
        logger.info("apply_function called")
        logger.info("message payload: %r", message.value())
        return True
      except Exception:
        logger.exception("apply_function failed")
        return False

    # Define a trigger that listens to an Apache Kafka message queue
    trigger = KafkaMessageQueueTrigger(
      topics=["test-topic"],
      apply_function="kafka.apply_function",
      kafka_config_id="kafka_conn",
      apply_function_args=None,
      apply_function_kwargs=None,
      poll_timeout=1,
      poll_interval=5,
    )

    # Define an asset that watches for messages on the queue
    asset = Asset("kafka_queue_asset", watchers=[AssetWatcher(name="kafka_watcher", trigger=trigger)])

    with DAG(dag_id="kafka_watcher", schedule=[asset]) as dag:
      EmptyOperator(task_id="task")

  triggerer.py: |
    from datetime import datetime, timedelta

    from airflow import DAG
    from airflow.models.baseoperator import BaseOperator
    from airflow.triggers.temporal import TimeDeltaTrigger
    from airflow.utils.context import Context
    from airflow.operators.empty import EmptyOperator

    # ------------------------------------------------------
    # Custom deferrable operator - does a simple async sleep
    # ------------------------------------------------------
    class CoreDeferrableSleepOperator(BaseOperator):
        """
        Sleeps for ``duration`` seconds without occupying a worker.
        The async hand-off happens via ``self.defer`` + ``TimeDeltaTrigger``.
        """
        ui_color = "#ffefeb"

        def __init__(self, *, duration: int, **kwargs):
            super().__init__(**kwargs)
            self.duration = duration

        def execute(self, context: Context):
            """Run on a worker, then hand control to the Triggerer."""
            # Build the trigger that will fire after `duration` seconds.
            trigger = TimeDeltaTrigger(timedelta(seconds=self.duration))

            # *** Asynchronous hand-off ***
            # This tells the scheduler: “pause this task, let the Triggerer watch the timer”.
            self.defer(trigger=trigger, method_name="execute_complete")

        def execute_complete(self, context: Context, event=None):
            """Resumes here once the Triggerer fires."""
            self.log.info("Deferrable sleep of %s seconds finished.", self.duration)
            return "DONE"

    default_args = {"owner": "stackable", "retries": 0}

    with DAG(
        dag_id="core_deferrable_sleep_demo",
        schedule=None,
        # N.B. this be earlier than the current timestamp!
        start_date=datetime(2025, 8, 1),
        catchup=False,
        default_args=default_args,
        tags=["example", "triggerer"],
    ) as dag:

        sleep = CoreDeferrableSleepOperator(
            task_id="deferrable_sleep",
            duration=10,
        )

        sleep
  date_demo.py: |
    """Example DAG returning the current date"""
    from datetime import datetime, timedelta

    from airflow import DAG
    from airflow.operators.bash import BashOperator

    with DAG(
        dag_id='date_demo',
        schedule='0-59 * * * *',
        start_date=datetime(2021, 1, 1),
        catchup=False,
        dagrun_timeout=timedelta(minutes=5),
        tags=['example'],
        params={},
    ) as dag:
        run_this = BashOperator(
            task_id='run_every_minute',
            bash_command='date',
        )
  dag_factory.py: |
    from airflow import DAG
    from airflow.operators.empty import EmptyOperator
    from datetime import datetime, timedelta

    # Number of DAGs to generate
    NUM_DAGS = 10  # Increase for more stress
    DAG_PREFIX = "stress_dag_"

    default_args = {
        'owner': 'airflow',
        'start_date': datetime(2025, 1, 1),
        'retries': 1,
        'retry_delay': timedelta(seconds=5),
    }

    def create_dag(dag_id):
        with DAG(
            dag_id=dag_id,
            default_args=default_args,
            schedule=None,
            catchup=False,
            tags=["stress_test"],
        ) as dag:
            start = EmptyOperator(task_id='start')
            end = EmptyOperator(task_id='end')
            start >> end
        return dag

    for i in range(NUM_DAGS):
        dag_id = f"{DAG_PREFIX}{i:04d}"
        globals()[dag_id] = create_dag(dag_id)
  pyspark_pi.py: |
    """Example DAG demonstrating how to apply a Kubernetes Resource from Airflow running in-cluster"""
    from datetime import datetime, timedelta
    from airflow import DAG
    from typing import TYPE_CHECKING, Optional, Sequence, Dict
    from kubernetes import client
    from airflow.exceptions import AirflowException
    from airflow.sensors.base import BaseSensorOperator
    from airflow.models import BaseOperator
    from airflow.providers.cncf.kubernetes.hooks.kubernetes import KubernetesHook
    import yaml
    from airflow.utils import yaml
    import os

    if TYPE_CHECKING:
        from airflow.utils.context import Context

    class SparkKubernetesOperator(BaseOperator):
        template_fields: Sequence[str] = ('application_file', 'namespace')
        template_ext: Sequence[str] = ('.yaml', '.yml', '.json')
        ui_color = '#f4a460'

        def __init__(
                self,
                *,
                application_file: str,
                namespace: Optional[str] = None,
                kubernetes_conn_id: str = 'kubernetes_in_cluster',
                api_group: str = 'spark.stackable.tech',
                api_version: str = 'v1alpha1',
                **kwargs,
        ) -> None:
            super().__init__(**kwargs)
            self.application_file = application_file
            self.namespace = namespace
            self.kubernetes_conn_id = kubernetes_conn_id
            self.api_group = api_group
            self.api_version = api_version
            self.plural = "sparkapplications"

        def execute(self, context: 'Context'):
            hook = KubernetesHook(conn_id=self.kubernetes_conn_id)
            self.log.info("Creating SparkApplication...")
            response = hook.create_custom_object(
                group=self.api_group,
                version=self.api_version,
                plural=self.plural,
                body=self.application_file,
                namespace=self.namespace,
            )
            return response


    class SparkKubernetesSensor(BaseSensorOperator):
        template_fields = ("application_name", "namespace")
        # See https://github.com/stackabletech/spark-k8s-operator/pull/460/files#diff-d737837121132af6b60f50279a78464b05dcfd06c05d1d090f4198a5e962b5f6R371
        # Unknown is set immediately so it must be excluded from the failed states.
        FAILURE_STATES = ("Failed")
        SUCCESS_STATES = ("Succeeded")

        def __init__(
                self,
                *,
                application_name: str,
                attach_log: bool = False,
                namespace: Optional[str] = None,
                kubernetes_conn_id: str = 'kubernetes_in_cluster',
                api_group: str = 'spark.stackable.tech',
                api_version: str = 'v1alpha1',
                poke_interval: float = 60,
                **kwargs,
        ) -> None:
            super().__init__(**kwargs)
            self.application_name = application_name
            self.attach_log = attach_log
            self.namespace = namespace
            self.kubernetes_conn_id = kubernetes_conn_id
            self.hook = KubernetesHook(conn_id=self.kubernetes_conn_id)
            self.api_group = api_group
            self.api_version = api_version
            self.poke_interval = poke_interval

        def _log_driver(self, application_state: str, response: dict) -> None:
            if not self.attach_log:
                return
            status_info = response["status"]
            if "driverInfo" not in status_info:
                return
            driver_info = status_info["driverInfo"]
            if "podName" not in driver_info:
                return
            driver_pod_name = driver_info["podName"]
            namespace = response["metadata"]["namespace"]
            log_method = self.log.error if application_state in self.FAILURE_STATES else self.log.info
            try:
                log = ""
                for line in self.hook.get_pod_logs(driver_pod_name, namespace=namespace):
                    log += line.decode()
                log_method(log)
            except client.rest.ApiException as e:
                self.log.warning(
                    "Could not read logs for pod %s. It may have been disposed.\n"
                    "Make sure timeToLiveSeconds is set on your SparkApplication spec.\n"
                    "underlying exception: %s",
                    driver_pod_name,
                    e,
                )

        def poke(self, context: Dict) -> bool:
            self.log.info("Poking: %s", self.application_name)
            response = self.hook.get_custom_object(
                group=self.api_group,
                version=self.api_version,
                plural="sparkapplications",
                name=self.application_name,
                namespace=self.namespace,
            )
            try:
                application_state = response["status"]["phase"]
            except KeyError:
                self.log.debug(f"SparkApplication status could not be established: {response}")
                return False
            if self.attach_log and application_state in self.FAILURE_STATES + self.SUCCESS_STATES:
                self._log_driver(application_state, response)
            if application_state in self.FAILURE_STATES:
                raise AirflowException(f"SparkApplication failed with state: {application_state}")
            elif application_state in self.SUCCESS_STATES:
                self.log.info("SparkApplication ended successfully")
                return True
            else:
                self.log.info("SparkApplication is still in state: %s", application_state)
                return False

    with DAG(
            dag_id='sparkapp_dag',
            schedule=None,
            start_date=datetime(2022, 1, 1),
            catchup=False,
            dagrun_timeout=timedelta(minutes=60),
            tags=['example'],
            params={"example_key": "example_value"},
    ) as dag:

        def load_body_to_dict(body):
            try:
                body_dict = yaml.safe_load(body)
            except yaml.YAMLError as e:
                raise AirflowException(f"Exception when loading resource definition: {e}\n")
            return body_dict

        yaml_path = os.path.join(os.environ.get('AIRFLOW__CORE__DAGS_FOLDER'), 'pyspark_pi.yaml')

        with open(yaml_path, 'r') as file:
            crd = file.read()
        with open('/run/secrets/kubernetes.io/serviceaccount/namespace', 'r') as file:
            ns = file.read()

        document=load_body_to_dict(crd)
        application_name='pyspark-pi-'+datetime.utcnow().strftime('%Y%m%d%H%M%S')
        document.update({'metadata': {'name': application_name, 'namespace': ns}})

        t1 = SparkKubernetesOperator(
            task_id='spark_pi_submit',
            namespace=ns,
            application_file=document,
            do_xcom_push=True,
            dag=dag,
        )

        t2 = SparkKubernetesSensor(
            task_id='spark_pi_monitor',
            namespace=ns,
            application_name="{{ task_instance.xcom_pull(task_ids='spark_pi_submit')['metadata']['name'] }}",
            poke_interval=5,
            dag=dag,
        )

        t1 >> t2
  pyspark_pi.yaml: |
    ---
    apiVersion: spark.stackable.tech/v1alpha1
    kind: SparkApplication
    metadata:
      name: pyspark-pi
    spec:
      version: "1.0"
      sparkImage:
        productVersion: 4.0.1
      mode: cluster
      mainApplicationFile: local:///stackable/spark/examples/src/main/python/pi.py
      job:
        config:
          resources:
            cpu:
              min: 500m
              max: 500m
            memory:
              limit: 512Mi
      driver:
        config:
          resources:
            cpu:
              min: 1000m
              max: 1200m
            memory:
              limit: 1024Mi
      executor:
        config:
          resources:
            cpu:
              min: 500m
              max: 1000m
            memory:
              limit: 1024Mi
        replicas: 3

# {% endraw %}
---
apiVersion: v1
kind: Secret
metadata:
  name: airflow-credentials
type: Opaque
stringData:
  adminUser.username: admin
  adminUser.firstname: Airflow
  adminUser.lastname: Admin
  adminUser.email: airflow@airflow.com
  adminUser.password: "{{ airflowAdminPassword }}"
  connections.secretKey: "{{ airflowSecretKey }}"
  connections.sqlalchemyDatabaseUri: postgresql+psycopg2://airflow:airflow@postgresql-airflow/airflow
