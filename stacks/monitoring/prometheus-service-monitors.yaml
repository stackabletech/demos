# Use something like this to check for metrics:
# count by (app_kubernetes_io_name, app_kubernetes_io_instance, pod) ({app_kubernetes_io_name!="",pod!=""})
#
# Products metrics covered by the ServiceMonitors below. The list also includes whether the
# ServiceMonitor scrapes native metrics or a statsd/JMX exporter.
#
# [x] Airflow - exporter
# [x] Druid - native
# [x] HBase - native
# [x] Hadoop HDFS - native
# [x] Hive - exporter
# [x] Kafka - exporter
# [x] NiFi 1 - native
# [x] NiFi 2 - native
# [x] OpenSearch - native
# [ ] Spark - native - partially done, see comment on it below
# [x] Superset - exporter
# [x] Trino - native
# [x] ZooKeeper - native
# [x] OPA - native
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: stackable
  labels:
    stackable.tech/vendor: Stackable
    release: prometheus
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      stackable.tech/vendor: Stackable
      prometheus.io/scrape: "true"
    matchExpressions:
      - key: app.kubernetes.io/name
        operator: In
        values:
          - airflow
          - druid
          - hive
          - kafka
          - nifi # This only works for NiFi 1, NiFi 2 works via stackable-generic
          - opa
          - superset
          - trino
          - zookeeper
  endpoints:
    - scheme: http
      port: metrics
      path: /metrics
  podTargetLabels:
    - app.kubernetes.io/name
    - app.kubernetes.io/instance
    - app.kubernetes.io/component
    - app.kubernetes.io/role-group
    - app.kubernetes.io/version
---
# Utilize `prometheus.io/scheme`, `prometheus.io/port`, `prometheus.io/path` annotations set by the operators
# to scrape all Stackable products.
# [x] Airflow - relabel drop filter on airflow container
# [x] Druid
# [x] HBase
# [X] Hadoop HDFS - relabel drop filter on empty container
# [x] Hive
# [~] Kafka - TODO: listener services have metrics?
# [x] NiFi 1 + 2
# [ ] OpenSearch
# [x] Spark: Connect, HistoryServer
# [x] Superset - relabel drop filter on superset container
# [x] Trino
# [x] ZooKeeper
# [x] OPA
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: stackable-generic
  labels:
    stackable.tech/vendor: Stackable
    release: prometheus
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      stackable.tech/vendor: Stackable
      prometheus.io/scrape: "true"
  endpoints:
    - relabelings:
        - sourceLabels:
            - __meta_kubernetes_pod_container_name
          # Pods show up twice due to multiple containers, we only keep the main / product container.
          # Except for Airflow and Superset, where we chose the metrics container (otherwise scheduler, worker etc.
          # which only have the metrics container are not getting picked up).
          # - airflow: airflow
          # - superset: superset
          # - empty: filter when container label does not exist: hdfs
          regex: ^(airflow|superset|)$
          action: drop
        - sourceLabels:
            - __meta_kubernetes_service_annotation_prometheus_io_scheme
          action: replace
          targetLabel: __scheme__
          regex: (https?)
        - sourceLabels:
            - __meta_kubernetes_service_annotation_prometheus_io_path
          action: replace
          targetLabel: __metrics_path__
          regex: (.+)
        - sourceLabels:
            - __meta_kubernetes_service_name
            - __meta_kubernetes_namespace
            - __meta_kubernetes_service_annotation_prometheus_io_port
          action: replace
          targetLabel: __address__
          regex: (.+);(.+);(\d+)
          # TODO: We could set the cluster domain via annotation as well and pick it up here.
          replacement: $1.$2.svc.cluster.local:$3
      tlsConfig:
        ca:
          secret:
            name: prometheus-tls-certificate
            key: ca.crt
        cert:
          secret:
            name: prometheus-tls-certificate
            key: tls.crt
        keySecret:
          name: prometheus-tls-certificate
          key: tls.key
  podTargetLabels:
    - app.kubernetes.io/name
    - app.kubernetes.io/instance
    - app.kubernetes.io/component
    - app.kubernetes.io/role-group
    - app.kubernetes.io/version
---
# We prefer the native metrics over the statsd-exporter
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: stackable-hdfs
  labels:
    stackable.tech/vendor: Stackable
    release: prometheus
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      stackable.tech/vendor: Stackable
      prometheus.io/scrape: "true"
      app.kubernetes.io/name: hdfs
  endpoints:
    - scheme: http
      port: http # Don't use the "metrics" exporter port, we want native metrics instead
      path: /prom
  podTargetLabels:
    - app.kubernetes.io/name
    - app.kubernetes.io/instance
    - app.kubernetes.io/component
    - app.kubernetes.io/role-group
    - app.kubernetes.io/version
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: stackable-hbase
  labels:
    stackable.tech/vendor: Stackable
    release: prometheus
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      stackable.tech/vendor: Stackable
      prometheus.io/scrape: "true"
      app.kubernetes.io/name: hbase
  endpoints:
    - scheme: http
      port: metrics
      path: /prometheus
  podTargetLabels:
    - app.kubernetes.io/name
    - app.kubernetes.io/instance
    - app.kubernetes.io/component
    - app.kubernetes.io/role-group
    - app.kubernetes.io/version
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: stackable-opensearch
  labels:
    stackable.tech/vendor: Stackable
    release: prometheus
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      stackable.tech/vendor: Stackable
      prometheus.io/scrape: "true"
      app.kubernetes.io/name: opensearch
  endpoints:
    - relabelings:
        - sourceLabels:
            - __meta_kubernetes_service_annotation_prometheus_io_scheme
          action: replace
          targetLabel: __scheme__
          regex: (https?)
        - sourceLabels:
            - __meta_kubernetes_service_annotation_prometheus_io_path
          action: replace
          targetLabel: __metrics_path__
          regex: (.+)
        # Use the FQDN instead of the IP address because the IP address
        # is not contained in the certificate.
        - sourceLabels:
            - __meta_kubernetes_pod_name
            - __meta_kubernetes_service_name
            - __meta_kubernetes_namespace
            - __meta_kubernetes_service_annotation_prometheus_io_port
          action: replace
          targetLabel: __address__
          regex: (.+);(.+);(.+);(\d+)
          replacement: $1.$2.$3.svc.cluster.local:$4
      tlsConfig:
        ca:
          secret:
            name: prometheus-tls-certificate
            key: ca.crt
  podTargetLabels:
    - app.kubernetes.io/name
    - app.kubernetes.io/instance
    - app.kubernetes.io/component
    - app.kubernetes.io/role-group
    - app.kubernetes.io/version
---
# spark-k8s-operator does not deploy any Services at all (at least for SparkApplications).
# We currently only scrape the driver, going forward we might want to scrape the executors as well.
# In the future we might also want to scrape SparkConnect and HistoryServers.
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: stackable-spark-application-driver
  labels:
    stackable.tech/vendor: Stackable
    release: prometheus
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      stackable.tech/vendor: Stackable
      prometheus.io/scrape: "true"
      app.kubernetes.io/name: spark-k8s
      spark-role: driver
  podMetricsEndpoints:
    - scheme: http
      port: spark-ui
      path: /metrics/prometheus
  podTargetLabels:
    - app.kubernetes.io/name
    - app.kubernetes.io/instance
    - app.kubernetes.io/component
    - app.kubernetes.io/role-group
    - app.kubernetes.io/version
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: stackable-minio-http
  labels:
    stackable.tech/vendor: Stackable
    release: prometheus
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      # stackable.tech/vendor: Stackable # This is not always set, e.g. missing in the nifi-kafka-druid-water-level-data demo
      app: minio
      monitoring: "true"
  endpoints:
    - scheme: http
      port: http
      path: /minio/v2/metrics/cluster
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: stackable-minio-https
  labels:
    stackable.tech/vendor: Stackable
    release: prometheus
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      # stackable.tech/vendor: Stackable # This is not always set, e.g. missing in the nifi-kafka-druid-water-level-data demo
      app: minio
      monitoring: "true"
  endpoints:
    - scheme: https
      port: https
      path: /minio/v2/metrics/cluster
      # Prevent "tls: failed to verify certificate: x509: cannot validate certificate for 100.96.234.154 because it doesn't contain any IP SANs"
      tlsConfig:
        insecureSkipVerify: true
