---
# Use something like this to check for metrics:
# count by (app_kubernetes_io_name, app_kubernetes_io_instance, pod) ({app_kubernetes_io_name!="",pod!=""})
#
# Products metrics covered by the ServiceMonitors below. The list also includes whether the
# ServiceMonitor scrapes native metrics or a statsd/JMX exporter.
#
#
# Utilize `prometheus.io/scheme`, `prometheus.io/port`, `prometheus.io/path` (and optionally `prometheus.io/clusterdomain`)
# annotations set by the operators to scrape all Stackable products.
#
# [x] Airflow - relabel drop filter on airflow container
# [x] Druid
# [x] HBase
# [X] Hadoop HDFS - relabel drop filter on empty container
# [x] Hive
# [x] Kafka - TODO: listener services have metrics?
# [x] NiFi 1 + 2
# [x] OpenSearch
# [x] Spark: Connect, HistoryServer
# [x] Superset - relabel drop filter on superset container
# [x] Trino
# [x] ZooKeeper
# [x] OPA
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: stackable
  labels:
    stackable.tech/vendor: Stackable
    release: prometheus
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      stackable.tech/vendor: Stackable
      prometheus.io/scrape: "true"
  endpoints:
    - relabelings:
        - sourceLabels:
            - __meta_kubernetes_pod_container_name
          # Pods show up twice due to multiple containers, we only keep the main / product container.
          # Except for Airflow and Superset, where we choose the metrics container.
          # - airflow: airflow
          # - superset: superset
          # - empty: filter when container label does not exist: hdfs
          regex: ^(airflow|superset|)$
          action: drop
          # Add empty label if not existing or pass-through existing value
        - sourceLabels:
            - __meta_kubernetes_service_annotation_prometheus_io_clusterdomain
          targetLabel: __tmp_clusterdomain__
          replacement: $1
          # Use default value if empty
        - sourceLabels:
            - __tmp_clusterdomain__
          targetLabel: __tmp_clusterdomain__
          regex: ^$
          replacement: "cluster.local"
          # Scheme and port extraction
        - sourceLabels:
            - __meta_kubernetes_service_annotation_prometheus_io_scheme
          targetLabel: __scheme__
          regex: (https?)
        - sourceLabels:
            - __meta_kubernetes_service_annotation_prometheus_io_path
          targetLabel: __metrics_path__
          regex: (.+)
          # Build metrics service address
        - sourceLabels:
            - __meta_kubernetes_service_name
            - __meta_kubernetes_namespace
            - __tmp_clusterdomain__
            - __meta_kubernetes_service_annotation_prometheus_io_port
          targetLabel: __address__
          regex: (.+);(.+);(.+);(\d+)
          # <service-name>.<namespace>.svc.<cluster-domain>:<port>
          replacement: $1.$2.svc.$3:$4
      tlsConfig:
        ca:
          secret:
            name: prometheus-tls-certificate
            key: ca.crt
        cert:
          secret:
            name: prometheus-tls-certificate
            key: tls.crt
        keySecret:
          name: prometheus-tls-certificate
          key: tls.key
  podTargetLabels:
    - app.kubernetes.io/name
    - app.kubernetes.io/instance
    - app.kubernetes.io/component
    - app.kubernetes.io/role-group
    - app.kubernetes.io/version
---
# We currently only scrape the driver, going forward we might want to scrape the executors as well.
# SparkConnect and HistoryServers are scraped via the `stackable` ServiceMonitor.
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: stackable-spark-application-driver
  labels:
    stackable.tech/vendor: Stackable
    release: prometheus
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      stackable.tech/vendor: Stackable
      prometheus.io/scrape: "true"
      app.kubernetes.io/name: spark-k8s
      spark-role: driver
  podMetricsEndpoints:
    - scheme: http
      port: spark-ui
      path: /metrics/prometheus
  podTargetLabels:
    - app.kubernetes.io/name
    - app.kubernetes.io/instance
    - app.kubernetes.io/component
    - app.kubernetes.io/role-group
    - app.kubernetes.io/version
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: stackable-minio-https
  labels:
    stackable.tech/vendor: Stackable
    release: prometheus
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      # stackable.tech/vendor: Stackable # This is not always set, e.g. missing in the nifi-kafka-druid-water-level-data demo
      app: minio
      monitoring: "true"
  endpoints:
    - scheme: https
      port: https
      path: /minio/v2/metrics/cluster
      # Prevent "tls: failed to verify certificate: x509: cannot validate certificate for 100.96.234.154 because it doesn't contain any IP SANs"
      tlsConfig:
        insecureSkipVerify: true
