---
apiVersion: airflow.stackable.tech/v1alpha1
kind: AirflowCluster
metadata:
  name: airflow
spec:
  image:
    # N.B. KubernetesPodOperator does not appear to work properly with
    # Airflow 3.x, returning a failed state even when the job is successful.
    productVersion: 2.10.5
    pullPolicy: IfNotPresent
  clusterConfig:
    loadExamples: false
    credentialsSecret: airflow-credentials
    volumes:
      - name: airflow-dags
        configMap:
          name: airflow-dags
    volumeMounts:
      - name: airflow-dags
        mountPath: /stackable/airflow/dags/dbt_trino.py
        subPath: dbt_trino.py
  webservers:
    roleConfig:
      listenerClass: external-stable
    envOverrides: &envOverrides
      #AIRFLOW_CONN_KUBERNETES_IN_CLUSTER: "kubernetes://?__extra__=%7B%22extra__kubernetes__in_cluster%22%3A+true%2C+%22extra__kubernetes__kube_config%22%3A+%22%22%2C+%22extra__kubernetes__kube_config_path%22%3A+%22%22%2C+%22extra__kubernetes__namespace%22%3A+%22%22%7D"
      #AIRFLOW_CONN_KUBERNETES_DEFAULT: "kubernetes://?__extra__=%7B%22extra__kubernetes__in_cluster%22%3A+true%2C+%22extra__kubernetes__kube_config%22%3A+%22%22%2C+%22extra__kubernetes__kube_config_path%22%3A+%22%22%2C+%22extra__kubernetes__namespace%22%3A+%22%22%7D"
    config:
      resources:
        cpu:
          min: "2"
          max: "3"
        memory:
          limit: 3Gi
    roleGroups:
      default:
        replicas: 1
  celeryExecutors:
    envOverrides: *envOverrides
    roleGroups:
      default:
        replicas: 1
  schedulers:
    envOverrides: *envOverrides
    roleGroups:
      default:
        replicas: 1
  # dagProcessors:
  #   envOverrides: *envOverrides
  #   roleGroups:
  #     default:
  #       replicas: 1
  # triggerers:
  #   envOverrides: *envOverrides
  #   roleGroups:
  #     default:
  #       replicas: 1
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-dags
data:
  dbt_trino.py: |
    from airflow import DAG
    from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator

    with DAG(
        dag_id="run_dbt_check_via_pod_operator",
        schedule=None,
        tags=["Demo", "DBT"],
        catchup=False
    ) as dag:
      run_dbt_check = KubernetesPodOperator(
          image="my-dbt-trino:0.0.1",
          image_pull_policy="IfNotPresent",
          cmds=["/bin/bash", "-x", "-euo", "pipefail", "-c"],
          arguments=["dbt debug"],
          name="run-dbt-debug",
          task_id="dbt-task",
          get_logs=True,
          startup_timeout_seconds=600
      )
      run_dbt_check
---
apiVersion: v1
kind: Secret
metadata:
  name: airflow-credentials
type: Opaque
stringData:
  adminUser.username: admin
  adminUser.firstname: Airflow
  adminUser.lastname: Admin
  adminUser.email: airflow@airflow.com
  adminUser.password: "adminadmin"
  connections.secretKey: "airflowSecretKey"
  connections.sqlalchemyDatabaseUri: postgresql+psycopg2://airflow:airflow@postgresql-airflow/airflow
  connections.celeryResultBackend: db+postgresql://airflow:airflow@postgresql-airflow/airflow
  connections.celeryBrokerUrl: redis://:redis@redis-airflow-master:6379/0
