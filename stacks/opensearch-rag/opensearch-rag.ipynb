{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch RAG Demo with Llama 3.1 8B\n",
    "\n",
    "This notebook demonstrates Retrieval Augmented Generation (RAG) using:\n",
    "- **[OpenSearch](https://opensearch.org/platform/vector-engine/)** for vector search (k-NN) and hybrid search (k-NN + BM25)\n",
    "- **Ollama with Llama 3.1 8B** for text generation\n",
    "- **nomic-embed-text:v1.5** for embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1walebjxo53",
   "metadata": {},
   "source": [
    "## About the Data\n",
    "\n",
    "This demo uses **~4200 documentation chunks** from the Stackable Data Platform, covering operator documentation for products like Kafka, Trino, OpenSearch, Spark, and others. Each chunk contains:\n",
    "\n",
    "- Text content from documentation pages\n",
    "- 768-dimensional vector embeddings (generated with nomic-embed-text)\n",
    "- Metadata: repository name, category, URL, code block indicators\n",
    "\n",
    "The data is pre-generated and loaded into OpenSearch during demo deployment. This allows you to immediately start querying without waiting for document processing or embedding generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2b0cf",
   "metadata": {},
   "source": [
    "### Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install opensearch-py requests urllib3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4531f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from opensearchpy import OpenSearch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration from environment variables\n",
    "OPENSEARCH_HOSTS = os.getenv('OPENSEARCH_HOSTS')\n",
    "OPENSEARCH_HOSTNAME = os.getenv('OPENSEARCH_HOSTNAME')\n",
    "OPENSEARCH_PORT = int(os.getenv('OPENSEARCH_PORT'))\n",
    "OPENSEARCH_PROTOCOL = os.getenv('OPENSEARCH_PROTOCOL')\n",
    "OPENSEARCH_USER = os.getenv('OPENSEARCH_USER')\n",
    "OPENSEARCH_PASSWORD = os.getenv('OPENSEARCH_PASSWORD')\n",
    "OLLAMA_HOST = os.getenv('OLLAMA_HOST')\n",
    "OLLAMA_PORT = int(os.getenv('OLLAMA_PORT'))\n",
    "OLLAMA_LLM_MODEL = os.getenv('OLLAMA_LLM_MODEL')\n",
    "INDEX_NAME = 'rag-documents'\n",
    "\n",
    "print(f\"OpenSearch: {OPENSEARCH_HOSTS}\")\n",
    "print(f\"Ollama: {OLLAMA_HOST}:{OLLAMA_PORT} using {OLLAMA_LLM_MODEL} to generate responses\")\n",
    "print(f\"Index: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ad1d3",
   "metadata": {},
   "source": [
    "### Connect to Services\n",
    "Initialize the OpenSearch client and test the connections to OpenSearch and Ollama.\n",
    "Verify that the index containing the documentation chunks exists in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dba02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenSearch client\n",
    "opensearch_client = OpenSearch(\n",
    "    hosts=[{'host': OPENSEARCH_HOSTNAME, 'port': OPENSEARCH_PORT}],\n",
    "    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),\n",
    "    use_ssl=OPENSEARCH_PROTOCOL == 'https',\n",
    "    verify_certs=False,\n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "info = opensearch_client.info()\n",
    "print(f\"Connected to OpenSearch {info['version']['number']}\")\n",
    "\n",
    "# Check index\n",
    "if opensearch_client.indices.exists(index=INDEX_NAME):\n",
    "    count = opensearch_client.count(index=INDEX_NAME)['count']\n",
    "    print(f\"Index '{INDEX_NAME}' has {count} chunks\")\n",
    "else:\n",
    "    print(f\"Index '{INDEX_NAME}' does not exist. Run the data ingestion job first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabe8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Ollama connection\n",
    "response = requests.get(f'http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/tags')\n",
    "models = response.json().get('models', [])\n",
    "print(f\"Ollama is running with {len(models)} models:\")\n",
    "for model in models:\n",
    "    print(f\"  - {model['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559b700",
   "metadata": {},
   "source": [
    "## Understanding the RAG Pipeline\n",
    "\n",
    "RAG (Retrieval Augmented Generation) enhances LLM responses by grounding them in specific documentation. Our pipeline follows 5 steps:\n",
    "\n",
    "1. **Query Embedding**: Convert the user's question into a vector representation\n",
    "2. **Query Enhancement**: Detect keywords and intent to improve search precision\n",
    "3. **OpenSearch Hybrid Search**: Combine semantic search (k-NN) with keyword matching (BM25) using OpenSearch\n",
    "4. **Context Formatting & Prompt Engineering**: Prepare retrieved documentation for the LLM\n",
    "5. **Response Generation**: Stream the answer from the LLM\n",
    "\n",
    "Each step is explained below with its implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bxj3p1tq9j",
   "metadata": {},
   "source": [
    "## Step 1: Query Embedding\n",
    "\n",
    "To search for relevant documentation, we need to convert text into numerical vectors (embeddings). These vectors capture semantic meaning - similar concepts have similar vectors.\n",
    "\n",
    "**Why nomic-embed-text?**\n",
    "- Specifically optimized for retrieval tasks (not just similarity)\n",
    "- 768-dimensional vectors balance quality and performance  \n",
    "- Open source and runs locally in Ollama\n",
    "\n",
    "The embedding vector from the query will be compared against document embeddings stored in **OpenSearch's k-NN index**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Generate embedding for text using Ollama.\"\"\"\n",
    "    response = requests.post(\n",
    "        f'http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/embeddings',\n",
    "        json={\n",
    "            'model': 'nomic-embed-text:v1.5',\n",
    "            'prompt': text\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()['embedding']\n",
    "\n",
    "print(\"Query embedding function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954854c9",
   "metadata": {},
   "source": [
    "## Step 2: Query Enhancement for Better Results\n",
    "\n",
    "Raw user queries aren't always optimal for search. We enhance queries using two domain-specific strategies:\n",
    "\n",
    "### Strategy 1: Product Name Detection\n",
    "When a query mentions \"Kafka\" or \"Trino\", we want documentation **about** that product, not just pages that mention it. We detect product keywords and add metadata filters to the search.\n",
    "\n",
    "### Strategy 2: Implementation Query Detection  \n",
    "When users ask \"how to\" questions, code examples become more valuable. We detect implementation queries and boost document chunks containing code blocks.\n",
    "\n",
    "These enhancements are specific to searching technical documentation and significantly improve result precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operator name mapping\n",
    "OPERATOR_MAP = {\n",
    "    'trino': 'trino-operator',\n",
    "    'airflow': 'airflow-operator',\n",
    "    'druid': 'druid-operator',\n",
    "    'hdfs': 'hdfs-operator',\n",
    "    'hbase': 'hbase-operator',\n",
    "    'hive': 'hive-operator',\n",
    "    'kafka': 'kafka-operator',\n",
    "    'nifi': 'nifi-operator',\n",
    "    'opensearch': 'opensearch-operator',\n",
    "    'spark': 'spark-k8s-operator',\n",
    "    'superset': 'superset-operator',\n",
    "    'zookeeper': 'zookeeper-operator',\n",
    "    'opa': 'opa-operator',\n",
    "    'secret': 'secret-operator',\n",
    "    'listener': 'listener-operator',\n",
    "    'commons': 'commons-operator'\n",
    "}\n",
    "\n",
    "def detect_operator(query):\n",
    "    \"\"\"Detect operator names in query for metadata filtering.\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    for keyword, operator in OPERATOR_MAP.items():\n",
    "        if keyword in query_lower:\n",
    "            return operator\n",
    "    return None\n",
    "\n",
    "def is_implementation_query(query):\n",
    "    \"\"\"Detect if query is asking for implementation details.\"\"\"\n",
    "    impl_keywords = ['how', 'deploy', 'configure', 'setup', 'install', 'create', 'implement', 'example']\n",
    "    query_lower = query.lower()\n",
    "    return any(keyword in query_lower for keyword in impl_keywords)\n",
    "\n",
    "print(\"Query enhancement functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vmtxhpf0tpq",
   "metadata": {},
   "source": [
    "## Step 3: OpenSearch Hybrid Search (k-NN + BM25)\n",
    "\n",
    "**OpenSearch** provides both vector search and keyword search in a single query. We use [OpenSearch's vector engine](https://opensearch.org/platform/vector-engine/) for k-NN similarity search combined with BM25 for keyword matching.\n",
    "\n",
    "**Hybrid search** combines two complementary techniques:\n",
    "\n",
    "1. **k-NN (semantic search)**: Finds documents conceptually similar to the query, even if they use different words\n",
    "2. **BM25 (keyword search)**: Finds exact term matches with TF-IDF weighting\n",
    "\n",
    "**Why hybrid?** Each method has strengths:\n",
    "- k-NN: Handles synonyms, paraphrases, conceptual similarity\n",
    "- BM25: Catches exact terminology, product names, specific features\n",
    "\n",
    "We use a two-phase approach:\n",
    "1. OpenSearch k-NN retrieves candidate documents (k*3 results for better coverage)\n",
    "2. BM25 rescores these candidates to boost keyword matches\n",
    "\n",
    "**Weight tuning**: 70% semantic (k-NN), 30% keyword (BM25). Semantic similarity matters more for documentation Q&A, but exact terms still help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vlh8ehpwmrr",
   "metadata": {},
   "source": [
    "### 3.1: Building the OpenSearch k-NN Query\n",
    "\n",
    "OpenSearch's k-NN query performs vector similarity search. If a product is detected in the query, we add a metadata filter to restrict results to that product's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6kt8bl6kr",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knn_query(query_embedding, detected_operator, k):\n",
    "    \"\"\"Build k-NN query with optional operator filter.\"\"\"\n",
    "    knn_query = {\n",
    "        'embedding': {\n",
    "            'vector': query_embedding,\n",
    "            'k': k * 3  # Get more candidates for rescoring\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add operator filter if operator detected\n",
    "    if detected_operator:\n",
    "        knn_query['embedding']['filter'] = {\n",
    "            'term': {'operator': detected_operator}\n",
    "        }\n",
    "    \n",
    "    return knn_query\n",
    "\n",
    "print(\"k-NN query builder loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gsrj2dqzcxd",
   "metadata": {},
   "source": [
    "### 3.2: Building the BM25 Rescore Query\n",
    "\n",
    "The rescore query uses BM25 for keyword matching. It searches in both `title` (with 1.5x boost) and `content` fields. For implementation queries, we additionally boost documents containing code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5svrbc61d1n",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rescore_query(query, boost_code_blocks):\n",
    "    \"\"\"Build BM25 rescore query for keyword matching.\"\"\"\n",
    "    rescore_should = [\n",
    "        {\n",
    "            'multi_match': {\n",
    "                'query': query,\n",
    "                'fields': ['title^1.5', 'content'],\n",
    "                'type': 'best_fields'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Boost documents with code blocks for implementation queries\n",
    "    if boost_code_blocks:\n",
    "        rescore_should.append({\n",
    "            'term': {\n",
    "                'has_code_block': {\n",
    "                    'value': True,\n",
    "                    'boost': 1.5\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'window_size': None,  # Will be set to k * 3\n",
    "        'query': {\n",
    "            'rescore_query': {\n",
    "                'bool': {\n",
    "                    'should': rescore_should\n",
    "                }\n",
    "            },\n",
    "            'query_weight': 0.7,         # Favor k-NN semantic similarity\n",
    "            'rescore_query_weight': 0.3  # Keyword matching has less weight\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"BM25 rescore query builder loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9v342awomo6",
   "metadata": {},
   "source": [
    "### 3.3: Formatting Search Results\n",
    "\n",
    "Extract the relevant fields from OpenSearch response and structure them for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdmm66xll2h",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_search_results(opensearch_response):\n",
    "    \"\"\"Extract relevant fields from OpenSearch response.\"\"\"\n",
    "    results = []\n",
    "    for hit in opensearch_response['hits']['hits']:\n",
    "        results.append({\n",
    "            'title': hit['_source']['title'],\n",
    "            'content': hit['_source']['content'],\n",
    "            'category': hit['_source']['category'],\n",
    "            'operator': hit['_source'].get('operator', ''),\n",
    "            'has_code_block': hit['_source'].get('has_code_block', False),\n",
    "            'url': hit['_source'].get('url', ''),\n",
    "            'score': hit['_score']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "print(\"Search result formatter loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roc8moqb6x9",
   "metadata": {},
   "source": [
    "### 3.4: Execute OpenSearch Hybrid Search\n",
    "\n",
    "The main search function orchestrates all the steps above: get embedding, detect enhancements, build queries, execute OpenSearch search, and format results. The constructed OpenSearch query is logged for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y1wrd5gztoi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, k=10, log_query=False):\n",
    "    \"\"\"Search for relevant document chunks using OpenSearch hybrid search (k-NN + BM25).\"\"\"\n",
    "    # Step 1: Get embedding\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Step 2: Detect enhancements\n",
    "    detected_operator = detect_operator(query)\n",
    "    boost_code_blocks = is_implementation_query(query)\n",
    "    \n",
    "    # Step 3: Build k-NN query\n",
    "    knn_query = build_knn_query(query_embedding, detected_operator, k)\n",
    "    \n",
    "    # Step 4: Build rescore query\n",
    "    rescore_query = build_rescore_query(query, boost_code_blocks)\n",
    "    rescore_query['window_size'] = k * 3\n",
    "    \n",
    "    # Step 5: Construct complete search body\n",
    "    search_body = {\n",
    "        'size': k,\n",
    "        'query': {\n",
    "            'knn': knn_query\n",
    "        },\n",
    "        'rescore': rescore_query,\n",
    "        '_source': ['title', 'content', 'category', 'operator', 'has_code_block', 'url']\n",
    "    }\n",
    "    \n",
    "    # Log the query if requested (with truncated vector for readability)\n",
    "    if log_query:\n",
    "    # Create display version with truncated vector\n",
    "        log_body = json.loads(json.dumps(search_body))  # Deep copy\n",
    "        vector = log_body['query']['knn']['embedding']['vector']\n",
    "        log_body['query']['knn']['embedding']['vector'] = [vector[0], vector[1], vector[2], \"...\", vector[-3], vector[-2], vector[-1]]\n",
    "        print(\"OpenSearch Query:\")\n",
    "        print(json.dumps(log_body, indent=2))\n",
    "        print()\n",
    "    \n",
    "    # Step 6: Execute search\n",
    "    response = opensearch_client.search(\n",
    "        index=INDEX_NAME,\n",
    "        body=search_body\n",
    "    )\n",
    "    \n",
    "    # Step 7: Format and return results\n",
    "    return format_search_results(response)\n",
    "\n",
    "print(\"OpenSearch hybrid search orchestrator loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4he7ze2ju9o",
   "metadata": {},
   "source": [
    "## Step 4: Context Formatting & Prompt Engineering\n",
    "\n",
    "Once we have relevant documentation chunks, we need to format them for the LLM and construct an effective prompt. This step is critical for preventing hallucinations and ensuring high-quality responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ln0hj4qo5tm",
   "metadata": {},
   "source": [
    "### 4.1: Format Context for LLM\n",
    "\n",
    "Each retrieved document is formatted with metadata (title, operator, relevance score, URL) to help the LLM understand the source quality and provide citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9mu8uy4dr9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_for_llm(docs):\n",
    "    \"\"\"Format retrieved documents into context string with metadata.\"\"\"\n",
    "    context_parts = []\n",
    "    for doc in docs:\n",
    "        context_parts.append(\n",
    "            f\"Source: {doc['title']} (from {doc['operator']}, relevance: {doc['score']:.2f})\\n\"\n",
    "            f\"URL: {doc.get('url', 'N/A')}\\n\"\n",
    "            f\"{doc['content']}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "print(\"Context formatter loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i6q35frh9i",
   "metadata": {},
   "source": [
    "### 4.2: Build RAG Prompt\n",
    "\n",
    "The prompt is the most important part of RAG. It instructs the LLM on how to use the retrieved context.\n",
    "\n",
    "**Key prompt engineering principles:**\n",
    "- **Constrain to sources**: Explicitly tell the LLM to use ONLY the provided documentation, reducing hallucination\n",
    "- **Prioritize by relevance**: Mention that higher relevance scores indicate better sources\n",
    "- **Enable citations**: Include URLs so the LLM can reference specific documentation pages\n",
    "- **Prevent URL fabrication**: Explicitly forbid making up URLs\n",
    "- **Natural language**: Instruct to say \"the documentation\" rather than \"the context\" for more natural responses\n",
    "- **Admit uncertainty**: Tell the LLM to say when the answer isn't in the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3y98w17otia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(query, context_text):\n",
    "    \"\"\"Construct the complete RAG prompt with instructions.\"\"\"\n",
    "    prompt = f\"\"\"You are a technical documentation assistant for Stackable Data Platform. Answer the question using ONLY the provided documentation sources.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "- Use only information from the documentation sources below\n",
    "- Focus on sources with higher relevance scores\n",
    "- Include documentation URLs for further reading where relevant\n",
    "- Do NOT make up URLs - only use URLs provided below\n",
    "- If the answer is not available in the documentation, say so clearly\n",
    "- Be concise and technical\n",
    "- When referring to your sources, use natural language like 'the documentation' rather than 'the context'\n",
    "\n",
    "Documentation sources:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "print(\"RAG prompt builder loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5qy9bse8te",
   "metadata": {},
   "source": [
    "## Step 5: Response Generation with Streaming\n",
    "\n",
    "The final step sends our prompt to Llama 3.1 8B running in Ollama. We use streaming so tokens appear as they're generated, providing immediate feedback rather than waiting for the complete response.\n",
    "\n",
    "**Why streaming?** Better user experience - users see progress and can start reading before the full response is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5mbkgog5ox3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context_docs, stream=True):\n",
    "    \"\"\"Generate response using Llama 3.1 8B with retrieved context.\"\"\"\n",
    "    # Format context\n",
    "    context_text = format_context_for_llm(context_docs)\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = build_rag_prompt(query, context_text)\n",
    "    \n",
    "    # Call Ollama\n",
    "    response = requests.post(\n",
    "        f'http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/generate',\n",
    "        json={\n",
    "            'model': OLLAMA_LLM_MODEL,\n",
    "            'prompt': prompt,\n",
    "            'stream': stream\n",
    "        },\n",
    "        stream=stream\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    if stream:\n",
    "        # Stream tokens as they're generated\n",
    "        full_response = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                chunk = json.loads(line)\n",
    "                if 'response' in chunk:\n",
    "                    token = chunk['response']\n",
    "                    print(token, end='', flush=True)\n",
    "                    full_response += token\n",
    "        print()  # New line after streaming\n",
    "        return full_response\n",
    "    else:\n",
    "        # Return complete response\n",
    "        return response.json()['response']\n",
    "\n",
    "print(\"Response generator loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bjbrrddono",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline\n",
    "\n",
    "The `rag_query()` function ties all steps together and provides user feedback at each stage. This is the main interface for asking questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6tbawkzl0gd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, k=10, stream=True, log_query=True):\n",
    "    \"\"\"Complete RAG workflow: search + generate.\"\"\"\n",
    "    print(f\"Query: {query}\")\n",
    "    print()\n",
    "    \n",
    "    # Show if operator detected\n",
    "    detected_operator = detect_operator(query)\n",
    "    if detected_operator:\n",
    "        print(f\"Detected operator: {detected_operator}\")\n",
    "    \n",
    "    print(\"Retrieving relevant chunks...\")\n",
    "    print()\n",
    "    \n",
    "    # Search with query logging\n",
    "    context_docs = search_documents(query, k=k, log_query=log_query)\n",
    "    \n",
    "    print(f\"Found {len(context_docs)} relevant chunks:\")\n",
    "    for i, doc in enumerate(context_docs, 1):\n",
    "        code_indicator = \" [code]\" if doc.get('has_code_block') else \"\"\n",
    "        print(f\"  {i}. {doc['title']} (score: {doc['score']:.3f}){code_indicator}\")\n",
    "        print(f\"     {doc['url']}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Generating response with Llama 3.1 8B...\")\n",
    "    print()\n",
    "    answer = generate_response(query, context_docs, stream=stream)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return {'query': query, 'context': context_docs, 'answer': answer}\n",
    "\n",
    "print(\"RAG pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "go1u6jt3ikm",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Run the examples below to see the RAG pipeline in action. By default, `log_query=True` so you can inspect the OpenSearch query structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477d3f2",
   "metadata": {},
   "source": [
    "### Example 1: Ask about supported Kafka versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d456ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_query(\"What versions of Kafka are supported by the Stackable Data Platform?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb7a5f6",
   "metadata": {},
   "source": [
    "### Example 2: Ask about NiFi's authentication methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_query(\"What authentication methods does NiFi support?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486f495",
   "metadata": {},
   "source": [
    "### Example 3: Ask about deploying a Trino cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b114636",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_query(\"How do I deploy a Trino cluster with Stackable? Give me a sample yaml file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed864f5a",
   "metadata": {},
   "source": [
    "### Try Your Own Questions\n",
    "\n",
    "Use the cell below to ask your own questions about the Stackable Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stream=True (default) to watch tokens generate in real-time\n",
    "# Use stream=False to display the complete response once it is generated\n",
    "# Modify k to change the number of documentation chunks that will be retrieved from OpenSearch to be used as context in the prompt.\n",
    "# Use log_query=False to stop logging the OpenSearch query.\n",
    "result = rag_query(\"YOUR QUESTION HERE\", stream=True, k=10, log_query=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
